{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now discuss replacing the dot product model with a deep-learning model. We now no longer require users and movies to be represented by latent factors of the same dimension, but it is still useful to represent users and movies in a given embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, it is useful to represent categorical data in an embedding. Recall that categorical features in \\(n\\) values can have a one-hot representation as \\(n\\) binary features. This manes it amenable to regression techniques, but that is a lot of factors and we are not using the dynamic range \\((0,1)\\) very well. We can do something intermediate and learn an embedding in a smaller number of factors, which makes better utilization of the dynamic range and also learn relationships between the embeddings themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastai` provides `get_emb_sz`, which when given a `Tabular` object, like a `TabularDataLoaders`, looks at the categorical features and for each such feature heuristically estimates a suggested dimensionality for an embedding of it based on the number of values in the category, and then returns these as a list of shape parameters for each category, where the shape parameters are those for a matrix that defines a linear map from each categorical value into a vector corresponding to its embedding. (Being a linear may, it is equivalent to an FC layer from one-hot inputs without an activation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this, we first repeat the work in previous notebooks up to defining the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# Dataset preparation\n",
    "path = untar_data(URLs.ML_100k)\n",
    "\n",
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                      names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n",
    "                     usecols=(0, 1), names=('movie_id', 'title'), header=None)\n",
    "ratings = ratings.merge(movies)\n",
    "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dls` is a `TabularDataLoaders` object, which supports being passed into `get_emb_sz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.tabular.data.TabularDataLoaders"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(944, 74), (1665, 102)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = get_emb_sz(dls)\n",
    "embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
