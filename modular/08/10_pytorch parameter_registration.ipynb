{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an aim to write our own `Embedding` model used in the previous models, we first delve more into registration of parameters with PyTorch; how to do it, and what is its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize/assign `tensor`s to an attribute, they are not automatically registered as a parameter of the model. Note that we query parameters with `model_instance.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.tabular.all import *\n",
    "\n",
    "class T(Module):\n",
    "    def __init__(self):\n",
    "        self.a = torch.ones(3)\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we wrap the objects to be registered in `nn.Parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [Parameter containing:\n",
       "tensor([1., 1., 1.], requires_grad=True)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class T(Module):\n",
    "    def __init__(self):\n",
    "        self.a = nn.Parameter(torch.ones(3))\n",
    "\n",
    "L(T().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point one wonders why we hadn't needed to do this when we assigned submodules `Embedding` in our `DotProduct`. Of course, `Embedding` is not a `tensor`. Rather, predefined PyTorch modules already register their relevant parameters. We illustrate thes below.\n",
    "\n",
    "Of course, such predefined modules may make use of additional (unregistered) tensors and state e.g. for hyperparameters and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Parameter containing:\n",
       "tensor([[-0.0827],\n",
       "        [ 0.9234],\n",
       "        [ 0.4008]], requires_grad=True),Parameter containing:\n",
       "tensor([-0.0371, -0.9097, -0.7345], requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class T(Module):\n",
    "    def __init__(self):\n",
    "        self.a = nn.Linear(1, 3)\n",
    "\n",
    "t = T()\n",
    "L(t.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predefined `Linear` layer stores its model weights in its `weight` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t.a.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary significance of registering parameters with PyTorch is that PyTorch will perform automatic differention on it when training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
